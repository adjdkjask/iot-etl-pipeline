# GuÃ­a de Inicio RÃ¡pido

CÃ³mo ejecutar el pipeline ETL de IoT completo.

## Prerrequisitos

- [Docker Desktop](https://www.docker.com/products/docker-desktop/) instalado y corriendo
- Archivo `.env` configurado con credenciales de la API (copiar de `.env.example`)

## OpciÃ³n A: EjecuciÃ³n manual (desarrollo)

Ideal para desarrollo, debugging y ejecuciones puntuales.

```bash
# 1. Construir imagen
docker-compose build

# 2. Ejecutar fases del pipeline

# Extract: descarga datos de API â†’ data/raw/
docker-compose run --rm etl python -m extract.extractor

# Transform: limpia y modela â†’ data/output/
docker-compose run --rm etl python -m transform.transformer

# Load: exporta a CSV â†’ data/exports/
docker-compose run --rm etl python -m load.loader

# 3. Verificar resultados
ls data/exports/
```

**Ventajas:** Simple, rÃ¡pido para iterar, no consume recursos extras.

ğŸ“– DocumentaciÃ³n detallada: [etl-docker.md](etl-docker.md)

---

## OpciÃ³n B: EjecuciÃ³n orquestada (producciÃ³n)

Ideal para ejecuciones programadas y monitoreo.

```bash
# 1. Construir imagen ETL (desde raÃ­z)
docker-compose build

# 2. Inicializar Airflow (solo primera vez)
cd orchestration
docker-compose up airflow-init

# 3. Levantar Airflow
docker-compose up -d

# 4. Abrir UI y activar DAG
#    http://localhost:8080 (admin/admin)
```

**Ventajas:** Scheduling automÃ¡tico, reintentos, logs centralizados, UI de monitoreo.

ğŸ“– DocumentaciÃ³n detallada: [orchestration-docker.md](orchestration-docker.md)

---

## Comparativa

| Aspecto | Manual | Orquestado |
|---------|--------|------------|
| Comando | `docker-compose run --rm etl` | Trigger desde UI/CLI |
| Scheduling | No | SÃ­ (`@daily`, cron, etc.) |
| Reintentos | No | SÃ­ (configurable) |
| Monitoreo | Logs en terminal | UI web + logs persistentes |
| Recursos | ~2.4GB (solo durante ejecuciÃ³n) | ~4GB (Airflow siempre corriendo) |
| Uso tÃ­pico | Desarrollo, testing | ProducciÃ³n, pipelines recurrentes |

---

## Flujo de datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚ â”€â”€â–¶ â”‚  Transform  â”‚ â”€â”€â–¶ â”‚    Load     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                   â”‚                   â”‚
      â–¼                   â–¼                   â–¼
  data/raw/          data/output/       data/exports/
  (Parquet)          (Parquet)          (CSV)
   Bronze              Gold              Export
```

---

## Verificar instalaciÃ³n

```bash
# Construir y ejecutar tests
docker-compose build
docker-compose run --rm test

# Resultado esperado: 94 tests passed
```

---

## Estructura del proyecto

```
iot-etl-pipeline/
â”œâ”€â”€ docker-compose.yml      # Contenedor ETL
â”œâ”€â”€ Dockerfile              # Imagen PySpark
â”œâ”€â”€ .env                    # Variables de entorno (no commitear)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ path_config.py      # Rutas centralizadas
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/            # Fase 1: ExtracciÃ³n
â”‚   â”œâ”€â”€ transform/          # Fase 2: TransformaciÃ³n
â”‚   â””â”€â”€ load/               # Fase 3: Carga
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Datos crudos
â”‚   â”œâ”€â”€ processed/          # Datos limpios
â”‚   â”œâ”€â”€ output/             # Modelo dimensional
â”‚   â””â”€â”€ exports/            # CSVs finales
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ docker-compose.yml  # Servicios Airflow
â”‚   â””â”€â”€ dags/               # Definiciones DAG
â””â”€â”€ docs/                   # Esta documentaciÃ³n
```

---

## Comandos frecuentes

```bash
# === Desarrollo ===
docker-compose build                              # Construir imagen
docker-compose run --rm test                      # Correr tests
docker-compose run --rm etl bash                  # Shell interactivo

# === Airflow ===
cd orchestration
docker-compose up -d                              # Levantar servicios
docker-compose logs -f airflow-scheduler          # Ver logs
docker-compose exec airflow-scheduler \
  airflow dags trigger iot_etl_pipeline           # Trigger manual
docker-compose down                               # Detener

# === Limpieza ===
docker-compose down -v --rmi local                # Reset completo
```
